{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Multitask Transformer: News Summarization + Categorization\n",
    "This notebook demonstrates how to build and train a multitask transformer model that performs both news **summarization** and **categorization** using a shared encoder architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BartTokenizer, BartModel, BartConfig\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum = load_dataset('xsum', split='train[:1%]')\n",
    "ag_news = load_dataset('ag_news', split='train[:1%]')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 2040)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ag_news), len(xsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, xsum_data, ag_data, tokenizer, max_len=512):\n",
    "        self.xsum_data = xsum_data\n",
    "        self.ag_data = ag_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.xsum_data), len(self.ag_data))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx % 2 == 0:  # Summarization task\n",
    "            item = self.xsum_data[idx % len(self.xsum_data)]\n",
    "            enc = self.tokenizer(item['document'], truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "            dec = self.tokenizer(item['summary'], truncation=True, padding='max_length', max_length=64, return_tensors='pt')\n",
    "            return {\n",
    "                'input_ids': enc['input_ids'].squeeze(0),\n",
    "                'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "                'labels': dec['input_ids'].squeeze(0),\n",
    "                'task': 'summarization'\n",
    "            }\n",
    "        else:  # Classification task\n",
    "            item = self.ag_data[idx % len(self.ag_data)]\n",
    "            enc = self.tokenizer(item['text'], truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "            return {\n",
    "                'input_ids': enc['input_ids'].squeeze(0),\n",
    "                'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "                'label_class': item['label'],\n",
    "                'task': 'classification'\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multitask_collate(batch):\n",
    "    # Separate summarization and classification items\n",
    "    summarization_batch = [item for item in batch if item['task'] == 'summarization']\n",
    "    classification_batch = [item for item in batch if item['task'] == 'classification']\n",
    "\n",
    "    collated = {}\n",
    "\n",
    "    if summarization_batch:\n",
    "        collated['summarization'] = {\n",
    "            'input_ids': torch.stack([item['input_ids'] for item in summarization_batch]),\n",
    "            'attention_mask': torch.stack([item['attention_mask'] for item in summarization_batch]),\n",
    "            'labels': torch.stack([item['labels'] for item in summarization_batch])\n",
    "        }\n",
    "\n",
    "    if classification_batch:\n",
    "        collated['classification'] = {\n",
    "            'input_ids': torch.stack([item['input_ids'] for item in classification_batch]),\n",
    "            'attention_mask': torch.stack([item['attention_mask'] for item in classification_batch]),\n",
    "            'label_class': torch.tensor([item['label_class'] for item in classification_batch])\n",
    "        }\n",
    "\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskBart(nn.Module):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super().__init__()\n",
    "        self.encoder = BartModel(config).get_encoder()\n",
    "        self.decoder = BartModel(config).get_decoder()\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size)\n",
    "        self.classifier = nn.Linear(config.d_model, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, decoder_input_ids=None, task='summarization', label_class=None):\n",
    "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        encoder_hidden_states = encoder_outputs[0]\n",
    "\n",
    "        if task == 'summarization':\n",
    "            decoder_outputs = self.decoder(\n",
    "                input_ids=decoder_input_ids,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=attention_mask\n",
    "            )\n",
    "            return self.lm_head(decoder_outputs[0])  # (batch, seq, vocab)\n",
    "        else:\n",
    "            cls_representation = encoder_hidden_states[:, 0, :]  # use first token\n",
    "            return self.classifier(cls_representation)  # (batch, num_labels)\n",
    "        \n",
    "    def generate(self, input_ids, attention_mask, max_length=64, eos_token_id=2):\n",
    "        \"\"\"\n",
    "        Greedy decoding for summarization\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        batch_size = input_ids.size(0)\n",
    "        decoder_input_ids = torch.full((batch_size, 1), tokenizer.bos_token_id, dtype=torch.long)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            decoder_outputs = self.decoder(\n",
    "                input_ids=decoder_input_ids,\n",
    "                encoder_hidden_states=self.encoder(input_ids=input_ids, attention_mask=attention_mask)[0],\n",
    "                encoder_attention_mask=attention_mask\n",
    "            )\n",
    "            logits = self.lm_head(decoder_outputs[0])  # (batch, seq_len, vocab)\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "            decoder_input_ids = torch.cat([decoder_input_ids, next_token], dim=-1)\n",
    "\n",
    "            # Stop decoding when all sequences have generated an <eos>\n",
    "            if torch.all(next_token.squeeze(-1) == eos_token_id):\n",
    "                break\n",
    "\n",
    "        return decoder_input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BartConfig.from_pretrained('facebook/bart-base')\n",
    "model = MultiTaskBart(config, num_labels=4)  # AG News has 4 classes\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "dataset = MultiTaskDataset(xsum, ag_news, tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=multitask_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch: {epoch+1}\")\n",
    "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Summarization\n",
    "        if 'summarization' in batch:\n",
    "            input_ids = batch['summarization']['input_ids']\n",
    "            attention_mask = batch['summarization']['attention_mask']\n",
    "            labels = batch['summarization']['labels']\n",
    "\n",
    "            output = model(input_ids, attention_mask, decoder_input_ids=labels, task='summarization')\n",
    "            loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "            loss = loss_fn(output.view(-1, output.size(-1)), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Classification\n",
    "        if 'classification' in batch:\n",
    "            input_ids = batch['classification']['input_ids']\n",
    "            attention_mask = batch['classification']['attention_mask']\n",
    "            label_class = batch['classification']['label_class']\n",
    "\n",
    "            output = model(input_ids, attention_mask, task='classification')\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(output, label_class)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer, and optimizer saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\surya\\anaconda3\\Lib\\site-packages\\transformers\\configuration_utils.py:397: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# saving the model\n",
    "import os\n",
    "\n",
    "SAVE_PATH = \"./multitask_model_checkpoint\"\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Save model weights\n",
    "torch.save(model.state_dict(), os.path.join(SAVE_PATH, \"model.pt\"))\n",
    "\n",
    "# Save optimizer state (optional, for resuming training)\n",
    "torch.save(optimizer.state_dict(), os.path.join(SAVE_PATH, \"optimizer.pt\"))\n",
    "\n",
    "# Save config and tokenizer\n",
    "config.save_pretrained(SAVE_PATH)\n",
    "tokenizer.save_pretrained(SAVE_PATH)\n",
    "\n",
    "print(\"Model, tokenizer, and optimizer saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the model and retrain it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load tokenizer and config\n",
    "# from transformers import BartTokenizer, BartConfig\n",
    "\n",
    "# # SAVE_PATH = \"./multitask_model_checkpoint\"\n",
    "\n",
    "# tokenizer = BartTokenizer.from_pretrained(SAVE_PATH)\n",
    "# config = BartConfig.from_pretrained(SAVE_PATH)\n",
    "\n",
    "# # Re-initialize model and optimizer\n",
    "# model = MultiTaskBart(config, num_labels=4)\n",
    "# model.load_state_dict(torch.load(os.path.join(SAVE_PATH, \"model.pt\")))\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "# optimizer.load_state_dict(torch.load(os.path.join(SAVE_PATH, \"optimizer.pt\")))\n",
    "\n",
    "# model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.3947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Use a DataLoader for test data\n",
    "test_data = load_dataset('ag_news', split='test[:1%]')\n",
    "tokenized = [tokenizer(x['text'], padding='max_length', truncation=True, max_length=512, return_tensors='pt') for x in test_data]\n",
    "labels = [x['label'] for x in test_data]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, sample in enumerate(tokenized):\n",
    "        input_ids = sample['input_ids']\n",
    "        attention_mask = sample['attention_mask']\n",
    "        output = model(input_ids, attention_mask, task='classification')\n",
    "        pred = torch.argmax(output, dim=1).item()\n",
    "        all_preds.append(pred)\n",
    "        all_labels.append(labels[i])\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Classification Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'document'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m references \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m test_data:\n\u001b[1;32m----> 9\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m'\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m     10\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'document'"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "model.eval()\n",
    "generated = []\n",
    "references = []\n",
    "\n",
    "for sample in test_data:\n",
    "    inputs = tokenizer(sample['document'], return_tensors='pt', truncation=True, padding='max_length', max_length=512)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    output_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=64)\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    generated.append(summary)\n",
    "    references.append(sample['summary'])\n",
    "\n",
    "results = rouge.compute(predictions=generated, references=references, use_stemmer=True)\n",
    "print(\"ROUGE-1:\", results[\"rouge1\"])\n",
    "print(\"ROUGE-2:\", results[\"rouge2\"])\n",
    "print(\"ROUGE-L:\", results[\"rougeL\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartConfig\n",
    "import os\n",
    "\n",
    "# Load tokenizer and config\n",
    "CHECKPOINT_PATH = \"./multitask_model_checkpoint\"\n",
    "tokenizer = BartTokenizer.from_pretrained(CHECKPOINT_PATH)\n",
    "config = BartConfig.from_pretrained(CHECKPOINT_PATH)\n",
    "\n",
    "# Define your custom model class again\n",
    "class MultiTaskBart(torch.nn.Module):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super().__init__()\n",
    "        from transformers import BartModel\n",
    "        self.encoder = BartModel(config).get_encoder()\n",
    "        self.decoder = BartModel(config).get_decoder()\n",
    "        self.lm_head = torch.nn.Linear(config.d_model, config.vocab_size)\n",
    "        self.classifier = torch.nn.Linear(config.d_model, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, decoder_input_ids=None, task='summarization', label_class=None):\n",
    "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        encoder_hidden_states = encoder_outputs[0]\n",
    "\n",
    "        if task == 'summarization':\n",
    "            decoder_outputs = self.decoder(\n",
    "                input_ids=decoder_input_ids,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=attention_mask\n",
    "            )\n",
    "            return self.lm_head(decoder_outputs[0])\n",
    "        else:\n",
    "            cls_representation = encoder_hidden_states[:, 0, :]\n",
    "            return self.classifier(cls_representation)\n",
    "\n",
    "    def generate(self, input_ids, attention_mask, max_length=64, eos_token_id=2):\n",
    "        self.eval()\n",
    "        batch_size = input_ids.size(0)\n",
    "        decoder_input_ids = torch.full((batch_size, 1), tokenizer.bos_token_id, dtype=torch.long)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            encoder_hidden = self.encoder(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "            decoder_outputs = self.decoder(\n",
    "                input_ids=decoder_input_ids,\n",
    "                encoder_hidden_states=encoder_hidden,\n",
    "                encoder_attention_mask=attention_mask\n",
    "            )\n",
    "            logits = self.lm_head(decoder_outputs[0])\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            decoder_input_ids = torch.cat([decoder_input_ids, next_token], dim=-1)\n",
    "\n",
    "            if torch.all(next_token.squeeze(-1) == eos_token_id):\n",
    "                break\n",
    "\n",
    "        return decoder_input_ids\n",
    "\n",
    "# Instantiate and load model weights\n",
    "model = MultiTaskBart(config, num_labels=4)\n",
    "model.load_state_dict(torch.load(os.path.join(CHECKPOINT_PATH, \"model.pt\")))\n",
    "model.eval()\n",
    "print(\"‚úÖ Model loaded.\")\n",
    "\n",
    "# ---------------------\n",
    "# Inference\n",
    "# ---------------------\n",
    "\n",
    "text = \"\"\"Heavy monsoon rains have caused severe flooding in several parts of the country, displacing thousands of people and damaging homes and infrastructure.\"\"\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# ---- Summarization ----\n",
    "with torch.no_grad():\n",
    "    summary_ids = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=64)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# ---- Classification ----\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], task=\"classification\")\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "label_map = {\n",
    "    0: \"World\",\n",
    "    1: \"Sports\",\n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\"\n",
    "}\n",
    "\n",
    "print(\"Summary:\", summary)\n",
    "print(\"Predicted Category:\", label_map[predicted_class])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets evaluate rouge_score absl-py scikit-learn pandas\n",
    "# to train the model for custom categories\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BartTokenizer, BartConfig\n",
    "from sklearn.metrics import accuracy_score\n",
    "import evaluate\n",
    "import os\n",
    "\n",
    "# ----- PATHS -----\n",
    "CHECKPOINT_PATH = \"./multitask_model_checkpoint\"\n",
    "CUSTOM_DATA_PATH = \"./custom_data.csv\"  # CSV file with 'text' and 'label' columns\n",
    "\n",
    "# ----- CUSTOM CATEGORIES -----\n",
    "custom_labels = ['disaster', 'sports', 'finance', 'entertainment']  # modify as needed\n",
    "label2id = {label: idx for idx, label in enumerate(custom_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "# ----- LOAD MODEL CONFIG + TOKENIZER -----\n",
    "tokenizer = BartTokenizer.from_pretrained(CHECKPOINT_PATH)\n",
    "config = BartConfig.from_pretrained(CHECKPOINT_PATH)\n",
    "\n",
    "# ----- DEFINE MODEL -----\n",
    "class MultiTaskBart(torch.nn.Module):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super().__init__()\n",
    "        from transformers import BartModel\n",
    "        self.encoder = BartModel(config).get_encoder()\n",
    "        self.decoder = BartModel(config).get_decoder()\n",
    "        self.lm_head = torch.nn.Linear(config.d_model, config.vocab_size)\n",
    "        self.classifier = torch.nn.Linear(config.d_model, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, decoder_input_ids=None, task='summarization'):\n",
    "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        encoder_hidden_states = encoder_outputs[0]\n",
    "\n",
    "        if task == 'summarization':\n",
    "            decoder_outputs = self.decoder(\n",
    "                input_ids=decoder_input_ids,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=attention_mask\n",
    "            )\n",
    "            return self.lm_head(decoder_outputs[0])\n",
    "        else:\n",
    "            cls_representation = encoder_hidden_states[:, 0, :]\n",
    "            return self.classifier(cls_representation)\n",
    "\n",
    "    def generate(self, input_ids, attention_mask, max_length=64, eos_token_id=2):\n",
    "        self.eval()\n",
    "        batch_size = input_ids.size(0)\n",
    "        decoder_input_ids = torch.full((batch_size, 1), tokenizer.bos_token_id, dtype=torch.long)\n",
    "        for _ in range(max_length):\n",
    "            enc_hidden = self.encoder(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "            decoder_outputs = self.decoder(\n",
    "                input_ids=decoder_input_ids,\n",
    "                encoder_hidden_states=enc_hidden,\n",
    "                encoder_attention_mask=attention_mask\n",
    "            )\n",
    "            logits = self.lm_head(decoder_outputs[0])\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            decoder_input_ids = torch.cat([decoder_input_ids, next_token], dim=-1)\n",
    "            if torch.all(next_token.squeeze(-1) == eos_token_id):\n",
    "                break\n",
    "        return decoder_input_ids\n",
    "\n",
    "# ----- LOAD MODEL -----\n",
    "model = MultiTaskBart(config, num_labels=num_labels)\n",
    "model.load_state_dict(torch.load(os.path.join(CHECKPOINT_PATH, \"model.pt\")))\n",
    "model.eval()\n",
    "\n",
    "# ----- LOAD CUSTOM DATA -----\n",
    "df = pd.read_csv(CUSTOM_DATA_PATH)\n",
    "df = df[df['label'].isin(custom_labels)]  # filter to allowed classes\n",
    "df['label_id'] = df['label'].map(label2id)\n",
    "\n",
    "# ----- CLASSIFICATION -----\n",
    "print(\"\\nüìö Classification on custom dataset...\")\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    text = row['text']\n",
    "    label_id = row['label_id']\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], task=\"classification\")\n",
    "        pred = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    all_preds.append(pred)\n",
    "    all_labels.append(label_id)\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(\"‚úÖ Classification Accuracy:\", acc)\n",
    "print(\"üßæ Predicted Labels:\", [id2label[p] for p in all_preds[:5]])\n",
    "\n",
    "# ----- SUMMARIZATION (OPTIONAL for same input) -----\n",
    "print(\"\\nüìù Sample summarization (first 3 rows)...\")\n",
    "for _, row in df.head(3).iterrows():\n",
    "    text = row['text']\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "        summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\nüìÑ Text: {text[:100]}...\")\n",
    "    print(f\"üìù Summary: {summary}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
